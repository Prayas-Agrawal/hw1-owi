{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTqQIAYS2VgM",
        "outputId": "722fd2b0-af3f-42fb-84f4-f9302c035f70"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/datasets/VLyb/WN18RR\n",
        "!wget https://download.microsoft.com/download/8/7/0/8700516A-AB3D-4850-B4BB-805C515AECE1/FB15K-237.2.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxxkViXo3fde",
        "outputId": "6b11ea55-721b-430f-aa15-88500e55f5b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/FB15K-237.2.zip\n",
            "  inflating: /content/FB15K-237/Release/MSR-LA_Data_Full Rights_FB15K-237 Knowledge Base Completion Dataset (2650).docx  \n",
            "  inflating: /content/FB15K-237/Release/README.txt  \n",
            "  inflating: /content/FB15K-237/Release/test.txt  \n",
            "  inflating: /content/FB15K-237/Release/text_cvsc.txt  \n",
            "  inflating: /content/FB15K-237/Release/text_emnlp.txt  \n",
            "  inflating: /content/FB15K-237/Release/train.txt  \n",
            "  inflating: /content/FB15K-237/Release/valid.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/FB15K-237.2.zip -d /content/FB15K-237"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1zsz7JakPrE-"
      },
      "outputs": [],
      "source": [
        "fb15kPathTrain = \"/content/FB15K-237/Release/train.txt\"\n",
        "fb15kPathTest = \"/content/FB15K-237/Release/test.txt\"\n",
        "fb15kPathVal = \"/content/FB15K-237/Release/valid.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auFpf--EBNKt"
      },
      "source": [
        "#Complex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6A3583Z3k69",
        "outputId": "9774da9b-2a24-4c01-b6c8-64b340cdab23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1.0+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_PzuB7c4B0i"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def visualize_graph(G, color):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
        "                     node_color=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_embedding(h, color, epoch=None, loss=None):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    h = h.detach().cpu().numpy()\n",
        "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
        "    if epoch is not None and loss is not None:\n",
        "        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "131Opj2__3uR"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import FB15k_237\n",
        "from torch_geometric.nn import ComplEx, DistMult, RotatE, TransE\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "path = os.path.join('data', 'FB15k')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D12k9dQb_L7",
        "outputId": "642704d0-f5e3-4524-9851-3b04c0a690c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/train.txt\n",
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/valid.txt\n",
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/test.txt\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_data = FB15k_237(path, split='train')[0].to(device)\n",
        "val_data = FB15k_237(path, split='val')[0].to(device)\n",
        "test_data = FB15k_237(path, split='test')[0].to(device)\n",
        "\n",
        "model = ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations=train_data.num_edge_types,\n",
        "    hidden_channels=50,\n",
        ").to(device)\n",
        "\n",
        "loader = model.loader(\n",
        "    head_index=train_data.edge_index[0],\n",
        "    rel_type=train_data.edge_type,\n",
        "    tail_index=train_data.edge_index[1],\n",
        "    batch_size=1000,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=0.001, weight_decay=1e-6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilIOB_YscYXb"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = total_examples = 0\n",
        "    for head_index, rel_type, tail_index in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.loss(head_index, rel_type, tail_index)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * head_index.numel()\n",
        "        total_examples += head_index.numel()\n",
        "    return total_loss / total_examples\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(data):\n",
        "    model.eval()\n",
        "    return model.test(\n",
        "        head_index=data.edge_index[0],\n",
        "        rel_type=data.edge_type,\n",
        "        tail_index=data.edge_index[1],\n",
        "        batch_size=20000,\n",
        "        k=10,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zyd76T7Zei-_"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, 501):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "    # if epoch % 25 == 0:\n",
        "    #     rank, mrr, hits = test(val_data)\n",
        "    #     print(f'Epoch: {epoch:03d}, Val Mean Rank: {rank:.2f}, '\n",
        "    #           f'Val MRR: {mrr:.4f}, Val Hits@10: {hits:.4f}')\n",
        "\n",
        "rank, mrr, hits_at_10 = test(test_data)\n",
        "print(f'Test Mean Rank: {rank:.2f}, Test MRR: {mrr:.4f}, '\n",
        "      f'Test Hits@10: {hits_at_10:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBalItaeBIK4"
      },
      "source": [
        "#Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "esM_w-GDelKJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "JfZYgL6FVK99"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, vocab_size:int, input_dim:int, num_heads:int, num_encoder_layers: int, ff_dim:int, dropout: float):\n",
        "    super().__init__()\n",
        "    # self.posEncoder = None # Check without pos encoding for now\n",
        "    self.embedding = nn.Embedding(vocab_size, input_dim)\n",
        "    encoderLayer = TransformerEncoderLayer(input_dim, num_heads, ff_dim, dropout, )\n",
        "    self.encoder = TransformerEncoder(encoderLayer, num_encoder_layers)\n",
        "    self.maskedGen = nn.Linear(input_dim, vocab_size)\n",
        "    self.score = nn.Linear(input_dim, 1)\n",
        "\n",
        "  def forward(self, x, mask, method):\n",
        "    embeds = self.embedding(x)\n",
        "    if(method == 0):\n",
        "      return self.score(self.encoder(embeds))\n",
        "    else:\n",
        "      return self.maskedGen(self.encoder(embeds))\n",
        "\n",
        "def read_data(path):\n",
        "  return pd.read_csv(path, delimiter=\"\\t\", names=[\"s\", \"r\", \"o\"])\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataPath, vocab, transform=None, method = 1):\n",
        "\n",
        "    self.dataset = read_data(dataPath)\n",
        "    self.transform = transform\n",
        "    self.vocab = vocab\n",
        "    self.method = method\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    point = self.dataset.iloc[idx]\n",
        "    sample = None\n",
        "    if(self.method == 1): # masked generation\n",
        "\n",
        "      sample = {\n",
        "          \"x\": torch.tensor(vocab([\"<cls>\", point[\"s\"], \"<sep1>\" ,point[\"r\"],\"<sep2>\" ,\"<mask>\", \"<end>\"])),\n",
        "          \"y\":  F.one_hot(torch.tensor(vocab([point[\"o\"]])[0]), len(vocab))\n",
        "      }\n",
        "    if(self.transform):\n",
        "      sample = self.transform(sample)\n",
        "    return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "fY4oI80--bvX"
      },
      "outputs": [],
      "source": [
        "config2 = {\n",
        "    \"train\": fb15kPathTrain,\n",
        "    \"test\": fb15kPathTest,\n",
        "    \"val\": fb15kPathVal,\n",
        "}\n",
        "\n",
        "config = config2\n",
        "\n",
        "def yieldTokens(data_iter):\n",
        "  for i,row in data_iter.iterrows():\n",
        "    yield [row[\"s\"], row[\"r\"], row[\"o\"]]\n",
        "\n",
        "def yieldSubjectsObjects(data_iter, vocab):\n",
        "  for i,row in data_iter.iterrows():\n",
        "    yield vocab([row[\"s\"], row[\"o\"]])\n",
        "\n",
        "data_iter = read_data(config[\"train\"])\n",
        "vocab = build_vocab_from_iterator(yieldTokens(data_iter), specials=[\"<cls>\", \"<sep1>\", \"<sep2>\", \"<mask>\", \"<end>\", \"<unk>\"])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "subObj = yieldSubjectsObjects(data_iter)\n",
        "subjects = subObj[0]\n",
        "objects = subObj[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "7JMvkQwezboE"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(config[\"train\"],vocab)\n",
        "val_dataset = CustomDataset(config[\"val\"],vocab)\n",
        "test_dataset = CustomDataset(config[\"test\"],vocab)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 16, num_workers=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size = 16, num_workers=2, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 16, num_workers=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HKuFUo26sLw",
        "outputId": "9c9abac5-4873-45c0-b0d7-a923057daa02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = Transformer(len(vocab), 32, 2, 2, 32, 0)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=0.001, weight_decay=1e-6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "xy-tnCyu8sB1"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, method):\n",
        "  model.train()\n",
        "  total_loss = total_examples = 0\n",
        "  for batchIndex, data in enumerate(loader):\n",
        "    x = data[\"x\"]\n",
        "    y = data[\"y\"]\n",
        "    out = model(x, None, method)\n",
        "    loss = criterion(out, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += float(loss.item())\n",
        "    total_examples += 1\n",
        "\n",
        "    if(batchIndex%50 == 0):\n",
        "      print(f\"epoch {epoch} | {batchIndex}/{len(loader)} | loss: {loss.item()}\")\n",
        "        # print(f\"epoch {epoch} | {total_examples}/{len(loader)} | loss: {total_loss / total_examples}\")\n",
        "  # return total_loss / total_examples\n",
        "\n",
        "@torch.no_grad()\n",
        "def val(model,loader):\n",
        "  model.eval()\n",
        "  total_loss = total_examples = 0\n",
        "  for batchIndex, data in enumerate(loader):\n",
        "    x = data[\"x\"]\n",
        "    y = data[\"y\"]\n",
        "    out = model(x, None, method)\n",
        "    loss = criterion(out, y)\n",
        "\n",
        "    total_loss += float(loss.item())\n",
        "    total_examples += 1\n",
        "    if(batchIndex%50 == 0):\n",
        "      print(f\"val epoch {epoch} | {batchIndex}/{len(loader)} | loss: {loss.item()}\")\n",
        "  return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KFIu37aMS9Nk",
        "outputId": "de5a133f-7c46-4241-e289-043e5c88695a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 | 0/17008 | loss: 2.0697970390319824\n",
            "epoch 0 | 50/17008 | loss: 1.4791767597198486\n",
            "epoch 0 | 100/17008 | loss: 1.1832345724105835\n",
            "epoch 0 | 150/17008 | loss: 0.9659128189086914\n",
            "epoch 0 | 200/17008 | loss: 0.8294748663902283\n",
            "epoch 0 | 250/17008 | loss: 0.7142818570137024\n",
            "epoch 0 | 300/17008 | loss: 0.629608154296875\n",
            "epoch 0 | 350/17008 | loss: 0.5630539059638977\n",
            "epoch 0 | 400/17008 | loss: 0.5139343738555908\n",
            "epoch 0 | 450/17008 | loss: 0.4645530581474304\n",
            "epoch 0 | 500/17008 | loss: 0.4176269471645355\n",
            "epoch 0 | 550/17008 | loss: 0.38573575019836426\n",
            "epoch 0 | 600/17008 | loss: 0.3586159646511078\n",
            "epoch 0 | 650/17008 | loss: 0.32957229018211365\n",
            "epoch 0 | 700/17008 | loss: 0.3071689009666443\n",
            "epoch 0 | 750/17008 | loss: 0.2885400950908661\n",
            "epoch 0 | 800/17008 | loss: 0.271639347076416\n",
            "epoch 0 | 850/17008 | loss: 0.2541411817073822\n",
            "epoch 0 | 900/17008 | loss: 0.2382342666387558\n",
            "epoch 0 | 950/17008 | loss: 0.22699777781963348\n",
            "epoch 0 | 1000/17008 | loss: 0.21522490680217743\n",
            "epoch 0 | 1050/17008 | loss: 0.20423778891563416\n",
            "epoch 0 | 1100/17008 | loss: 0.193096324801445\n",
            "epoch 0 | 1150/17008 | loss: 0.18388967216014862\n",
            "epoch 0 | 1200/17008 | loss: 0.1760096549987793\n",
            "epoch 0 | 1250/17008 | loss: 0.16829624772071838\n",
            "epoch 0 | 1300/17008 | loss: 0.16456209123134613\n",
            "epoch 0 | 1350/17008 | loss: 0.15434153378009796\n",
            "epoch 0 | 1400/17008 | loss: 0.14786861836910248\n",
            "epoch 0 | 1450/17008 | loss: 0.14250032603740692\n",
            "epoch 0 | 1500/17008 | loss: 0.13660891354084015\n",
            "epoch 0 | 1550/17008 | loss: 0.13301587104797363\n",
            "epoch 0 | 1600/17008 | loss: 0.1276353895664215\n",
            "epoch 0 | 1650/17008 | loss: 0.12306316196918488\n",
            "epoch 0 | 1700/17008 | loss: 0.11882945150136948\n",
            "epoch 0 | 1750/17008 | loss: 0.11507387459278107\n",
            "epoch 0 | 1800/17008 | loss: 0.11180385947227478\n",
            "epoch 0 | 1850/17008 | loss: 0.10794507712125778\n",
            "epoch 0 | 1900/17008 | loss: 0.10460205376148224\n",
            "epoch 0 | 1950/17008 | loss: 0.10162799060344696\n",
            "epoch 0 | 2000/17008 | loss: 0.09889043867588043\n",
            "epoch 0 | 2050/17008 | loss: 0.09593934565782547\n",
            "epoch 0 | 2100/17008 | loss: 0.09340119361877441\n",
            "epoch 0 | 2150/17008 | loss: 0.09124131500720978\n",
            "epoch 0 | 2200/17008 | loss: 0.08833903819322586\n",
            "epoch 0 | 2250/17008 | loss: 0.08641350269317627\n",
            "epoch 0 | 2300/17008 | loss: 0.0841073989868164\n",
            "epoch 0 | 2350/17008 | loss: 0.08193414658308029\n",
            "epoch 0 | 2400/17008 | loss: 0.08047500252723694\n",
            "epoch 0 | 2450/17008 | loss: 0.07813577353954315\n",
            "epoch 0 | 2500/17008 | loss: 0.07674960047006607\n",
            "epoch 0 | 2550/17008 | loss: 0.0750318095088005\n",
            "epoch 0 | 2600/17008 | loss: 0.07317999005317688\n",
            "epoch 0 | 2650/17008 | loss: 0.07145167887210846\n",
            "epoch 0 | 2700/17008 | loss: 0.06995490193367004\n",
            "epoch 0 | 2750/17008 | loss: 0.06856752187013626\n",
            "epoch 0 | 2800/17008 | loss: 0.06790141761302948\n",
            "epoch 0 | 2850/17008 | loss: 0.06575987488031387\n",
            "epoch 0 | 2900/17008 | loss: 0.06451421231031418\n",
            "epoch 0 | 2950/17008 | loss: 0.06363509595394135\n",
            "epoch 0 | 3000/17008 | loss: 0.06218740716576576\n",
            "epoch 0 | 3050/17008 | loss: 0.061065223067998886\n",
            "epoch 0 | 3100/17008 | loss: 0.05975347012281418\n",
            "epoch 0 | 3150/17008 | loss: 0.059160858392715454\n",
            "epoch 0 | 3200/17008 | loss: 0.05760662257671356\n",
            "epoch 0 | 3250/17008 | loss: 0.056881483644247055\n",
            "epoch 0 | 3300/17008 | loss: 0.0556713305413723\n",
            "epoch 0 | 3350/17008 | loss: 0.05493154376745224\n",
            "epoch 0 | 3400/17008 | loss: 0.05403043329715729\n",
            "epoch 0 | 3450/17008 | loss: 0.05296875163912773\n",
            "epoch 0 | 3500/17008 | loss: 0.05208330973982811\n",
            "epoch 0 | 3550/17008 | loss: 0.051542799919843674\n",
            "epoch 0 | 3600/17008 | loss: 0.05044505000114441\n",
            "epoch 0 | 3650/17008 | loss: 0.0499136857688427\n",
            "epoch 0 | 3700/17008 | loss: 0.04892635717988014\n",
            "epoch 0 | 3750/17008 | loss: 0.0481688529253006\n",
            "epoch 0 | 3800/17008 | loss: 0.047545045614242554\n",
            "epoch 0 | 3850/17008 | loss: 0.04698757827281952\n",
            "epoch 0 | 3900/17008 | loss: 0.04606500640511513\n",
            "epoch 0 | 3950/17008 | loss: 0.04559694603085518\n",
            "epoch 0 | 4000/17008 | loss: 0.04482240974903107\n",
            "epoch 0 | 4050/17008 | loss: 0.044331684708595276\n",
            "epoch 0 | 4100/17008 | loss: 0.04361308738589287\n",
            "epoch 0 | 4150/17008 | loss: 0.04337730631232262\n",
            "epoch 0 | 4200/17008 | loss: 0.042490359395742416\n",
            "epoch 0 | 4250/17008 | loss: 0.041803330183029175\n",
            "epoch 0 | 4300/17008 | loss: 0.041224028915166855\n",
            "epoch 0 | 4350/17008 | loss: 0.04075317829847336\n",
            "epoch 0 | 4400/17008 | loss: 0.040207792073488235\n",
            "epoch 0 | 4450/17008 | loss: 0.03978091478347778\n",
            "epoch 0 | 4500/17008 | loss: 0.03934457525610924\n",
            "epoch 0 | 4550/17008 | loss: 0.03906041011214256\n",
            "epoch 0 | 4600/17008 | loss: 0.03829989209771156\n",
            "epoch 0 | 4650/17008 | loss: 0.037747859954833984\n",
            "epoch 0 | 4700/17008 | loss: 0.03735780715942383\n",
            "epoch 0 | 4750/17008 | loss: 0.03695813938975334\n",
            "epoch 0 | 4800/17008 | loss: 0.03648786619305611\n",
            "epoch 0 | 4850/17008 | loss: 0.036079291254282\n",
            "epoch 0 | 4900/17008 | loss: 0.0357058160007\n",
            "epoch 0 | 4950/17008 | loss: 0.03529946133494377\n",
            "epoch 0 | 5000/17008 | loss: 0.034989695996046066\n",
            "epoch 0 | 5050/17008 | loss: 0.034696586430072784\n",
            "epoch 0 | 5100/17008 | loss: 0.03412725403904915\n",
            "epoch 0 | 5150/17008 | loss: 0.03414914384484291\n",
            "epoch 0 | 5200/17008 | loss: 0.033496588468551636\n",
            "epoch 0 | 5250/17008 | loss: 0.03299225866794586\n",
            "epoch 0 | 5300/17008 | loss: 0.03266093134880066\n",
            "epoch 0 | 5350/17008 | loss: 0.03227946162223816\n",
            "epoch 0 | 5400/17008 | loss: 0.032155413180589676\n",
            "epoch 0 | 5450/17008 | loss: 0.03169463947415352\n",
            "epoch 0 | 5500/17008 | loss: 0.03132067620754242\n",
            "epoch 0 | 5550/17008 | loss: 0.03115573525428772\n",
            "epoch 0 | 5600/17008 | loss: 0.03072516992688179\n",
            "epoch 0 | 5650/17008 | loss: 0.03041110187768936\n",
            "epoch 0 | 5700/17008 | loss: 0.030130015686154366\n",
            "epoch 0 | 5750/17008 | loss: 0.029830986633896828\n",
            "epoch 0 | 5800/17008 | loss: 0.029553759843111038\n",
            "epoch 0 | 5850/17008 | loss: 0.02940639853477478\n",
            "epoch 0 | 5900/17008 | loss: 0.02914946712553501\n",
            "epoch 0 | 5950/17008 | loss: 0.028748303651809692\n",
            "epoch 0 | 6000/17008 | loss: 0.028484849259257317\n",
            "epoch 0 | 6050/17008 | loss: 0.028285225853323936\n",
            "epoch 0 | 6100/17008 | loss: 0.028098206967115402\n",
            "epoch 0 | 6150/17008 | loss: 0.027745883911848068\n",
            "epoch 0 | 6200/17008 | loss: 0.02747044339776039\n",
            "epoch 0 | 6250/17008 | loss: 0.02726871147751808\n",
            "epoch 0 | 6300/17008 | loss: 0.02700185775756836\n",
            "epoch 0 | 6350/17008 | loss: 0.026767492294311523\n",
            "epoch 0 | 6400/17008 | loss: 0.026563959196209908\n",
            "epoch 0 | 6450/17008 | loss: 0.02636837586760521\n",
            "epoch 0 | 6500/17008 | loss: 0.02606261521577835\n",
            "epoch 0 | 6550/17008 | loss: 0.025872377678751945\n",
            "epoch 0 | 6600/17008 | loss: 0.02558925375342369\n",
            "epoch 0 | 6650/17008 | loss: 0.025551600381731987\n",
            "epoch 0 | 6700/17008 | loss: 0.02521437779068947\n",
            "epoch 0 | 6750/17008 | loss: 0.025012990459799767\n",
            "epoch 0 | 6800/17008 | loss: 0.02497589774429798\n",
            "epoch 0 | 6850/17008 | loss: 0.024645550176501274\n",
            "epoch 0 | 6900/17008 | loss: 0.024401171132922173\n",
            "epoch 0 | 6950/17008 | loss: 0.02422533929347992\n",
            "epoch 0 | 7000/17008 | loss: 0.024031272158026695\n",
            "epoch 0 | 7050/17008 | loss: 0.02384786494076252\n",
            "epoch 0 | 7100/17008 | loss: 0.02373003587126732\n",
            "epoch 0 | 7150/17008 | loss: 0.023516125977039337\n",
            "epoch 0 | 7200/17008 | loss: 0.023354079574346542\n",
            "epoch 0 | 7250/17008 | loss: 0.023175397887825966\n",
            "epoch 0 | 7300/17008 | loss: 0.022969670593738556\n",
            "epoch 0 | 7350/17008 | loss: 0.022733410820364952\n",
            "epoch 0 | 7400/17008 | loss: 0.02267766371369362\n",
            "epoch 0 | 7450/17008 | loss: 0.022408906370401382\n",
            "epoch 0 | 7500/17008 | loss: 0.022345587611198425\n",
            "epoch 0 | 7550/17008 | loss: 0.022137243300676346\n",
            "epoch 0 | 7600/17008 | loss: 0.021932177245616913\n",
            "epoch 0 | 7650/17008 | loss: 0.021870672702789307\n",
            "epoch 0 | 7700/17008 | loss: 0.021693382412195206\n",
            "epoch 0 | 7750/17008 | loss: 0.021546045318245888\n",
            "epoch 0 | 7800/17008 | loss: 0.021407296881079674\n",
            "epoch 0 | 7850/17008 | loss: 0.02120530977845192\n",
            "epoch 0 | 7900/17008 | loss: 0.021120436489582062\n",
            "epoch 0 | 7950/17008 | loss: 0.020897308364510536\n",
            "epoch 0 | 8000/17008 | loss: 0.020769622176885605\n",
            "epoch 0 | 8050/17008 | loss: 0.020629484206438065\n",
            "epoch 0 | 8100/17008 | loss: 0.02048441395163536\n",
            "epoch 0 | 8150/17008 | loss: 0.020473426207900047\n",
            "epoch 0 | 8200/17008 | loss: 0.020220894366502762\n",
            "epoch 0 | 8250/17008 | loss: 0.0201253704726696\n",
            "epoch 0 | 8300/17008 | loss: 0.01996082067489624\n",
            "epoch 0 | 8350/17008 | loss: 0.019847866147756577\n",
            "epoch 0 | 8400/17008 | loss: 0.019670220091938972\n",
            "epoch 0 | 8450/17008 | loss: 0.019790222868323326\n",
            "epoch 0 | 8500/17008 | loss: 0.019417142495512962\n",
            "epoch 0 | 8550/17008 | loss: 0.019324783235788345\n",
            "epoch 0 | 8600/17008 | loss: 0.019261164590716362\n",
            "epoch 0 | 8650/17008 | loss: 0.019089970737695694\n",
            "epoch 0 | 8700/17008 | loss: 0.01898900419473648\n",
            "epoch 0 | 8750/17008 | loss: 0.018844228237867355\n",
            "epoch 0 | 8800/17008 | loss: 0.018847987055778503\n",
            "epoch 0 | 8850/17008 | loss: 0.01858973503112793\n",
            "epoch 0 | 8900/17008 | loss: 0.01847206801176071\n",
            "epoch 0 | 8950/17008 | loss: 0.018507953733205795\n",
            "epoch 0 | 9000/17008 | loss: 0.01829865388572216\n",
            "epoch 0 | 9050/17008 | loss: 0.018167881295084953\n",
            "epoch 0 | 9100/17008 | loss: 0.018074756488204002\n",
            "epoch 0 | 9150/17008 | loss: 0.01795249991118908\n",
            "epoch 0 | 9200/17008 | loss: 0.017919978126883507\n",
            "epoch 0 | 9250/17008 | loss: 0.01776387169957161\n",
            "epoch 0 | 9300/17008 | loss: 0.017670094966888428\n",
            "epoch 0 | 9350/17008 | loss: 0.017485953867435455\n",
            "epoch 0 | 9400/17008 | loss: 0.01748719811439514\n",
            "epoch 0 | 9450/17008 | loss: 0.01736319065093994\n",
            "epoch 0 | 9500/17008 | loss: 0.017289165407419205\n",
            "epoch 0 | 9550/17008 | loss: 0.01724742352962494\n",
            "epoch 0 | 9600/17008 | loss: 0.01706775464117527\n",
            "epoch 0 | 9650/17008 | loss: 0.016986165195703506\n",
            "epoch 0 | 9700/17008 | loss: 0.016841286793351173\n",
            "epoch 0 | 9750/17008 | loss: 0.01674155332148075\n",
            "epoch 0 | 9800/17008 | loss: 0.016698231920599937\n",
            "epoch 0 | 9850/17008 | loss: 0.016702774912118912\n",
            "epoch 0 | 9900/17008 | loss: 0.016507012769579887\n",
            "epoch 0 | 9950/17008 | loss: 0.016436472535133362\n",
            "epoch 0 | 10000/17008 | loss: 0.016364242881536484\n",
            "epoch 0 | 10050/17008 | loss: 0.016253851354122162\n",
            "epoch 0 | 10100/17008 | loss: 0.016149725764989853\n",
            "epoch 0 | 10150/17008 | loss: 0.016039658337831497\n",
            "epoch 0 | 10200/17008 | loss: 0.015987016260623932\n",
            "epoch 0 | 10250/17008 | loss: 0.015875954180955887\n",
            "epoch 0 | 10300/17008 | loss: 0.015815628692507744\n",
            "epoch 0 | 10350/17008 | loss: 0.015812205150723457\n",
            "epoch 0 | 10400/17008 | loss: 0.015658188611268997\n",
            "epoch 0 | 10450/17008 | loss: 0.015598677098751068\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-d21a0edae3e2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-660468b61fd6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, method)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 3\n",
        "method = 1\n",
        "for epoch in range(epochs):\n",
        "  train(model, train_dataloader, method)\n",
        "  val(model, val_dataloader)\n",
        "  print('-' * 89)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM2tK6rzTQDa"
      },
      "outputs": [],
      "source": [
        "  # emb size mismatch\n",
        "# train loop\n",
        "# loss maybe\n",
        "\n",
        "# method 0\n",
        "\n",
        "def hits():\n",
        "  for batch in test_dataloader:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
